from typing import List, Dict, Union
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
import time

import requests

SCROLL_PAUSE_TIME = 0.5
PARSE_INTERVAL = 5  # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–µ 5 —Å–∫—Ä–æ–ª–ª–æ–≤

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
months = {
    '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
    '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
    '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
}

today = datetime.today()
yesterday = today - timedelta(days=1)

DATE_RANGE = ["2025-05-09", "2025-05-10"]
class InterFaxParser:
    def __init__(self, url, driver, date_range, pattern=None):
        self.url = url
        self.driver = driver
        self.TIMEOUT = 0.1
        self.MAX_WAIT_TIME = 20
        self.date_start, self.date_end = date_range
        self.SOURCE_NAME = "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å"
        self.BASE_URL = "https://www.interfax.ru"
        self.window = None

        self.pattern_key_words = None

        if pattern:
            self.pattern_key_words = pattern
        print(self.pattern_key_words)
        self.news: List[Dict] = []
        # self.processed_urls = set()
        self.session = requests.Session()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def __parse_date(self, date_str: str) -> Union[datetime.date, None]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        date_str = date_str.strip()

        if re.match(r'^\d{2}:\d{2}$', date_str):
            return today.date(), date_str

        try:
            if date_str.startswith('–í—á–µ—Ä–∞'):
                return yesterday.date(), date_str.split(",")[1].strip()

            if ',' in date_str:
                date_part, time_part = date_str.split(',', 1)
                date_part = date_part.strip()

                if ' ' in date_part:
                    day, month_name = date_part.split()
                    if month_name in months:
                        month = months[month_name]
                        year = today.year
                        return datetime.strptime(f'{day}-{month}-{year}', '%d-%m-%Y').date(), date_str.split(",")[1].strip()

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã: {date_str} - {e}")
        return None

    def __extract_news_items(self, html: str, flag_parse_all = False) -> List[Dict]:
        processed_urls = set()
        dir_names = ["group", "photo", "text"]
        soup = BeautifulSoup(html, 'html.parser')
        groups = [soup.find_all("div", class_=f'timeline__{i}') for i in dir_names]
        extracted = []

        if not flag_parse_all:
            news_blocks = groups[0][-1].find_all("div", recursive=False)
            for block in news_blocks:
                time_tag = block.find("time")
                date_str = time_tag.get("datetime")
                article_datetime = datetime.strptime(date_str.split("T")[0], '%Y-%m-%d').date()
                if article_datetime < self.date_start:
                    return False  # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å while
        else:
            for group_type, groups_ in zip(dir_names, groups):
                for group in groups_:
                    # –î–ª—è timeline__group –∏—â–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ div, –¥–ª—è –¥—Ä—É–≥–∏—Ö - —Ä–∞–±–æ—Ç–∞–µ–º —Å —Å–∞–º–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
                    news_blocks = group.find_all("div", recursive=False) if group_type == "group" else [group]

                    for block in news_blocks:
                        time_tag = block.find("time")
                        a_tags = block.find_all("a")

                        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (h3)
                        h3_tag = block.find("h3")
                        if not h3_tag:
                            continue

                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Å—ã–ª–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        main_a_tag = None
                        for a in a_tags:
                            if a.find("h3"):
                                main_a_tag = a
                                break

                        if not main_a_tag:
                            continue

                        url = main_a_tag.get("href")
                        title = h3_tag.get_text(strip=True)
                        date_str = time_tag.get("datetime")

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏
                        if url in processed_urls:
                            continue
                        processed_urls.add(url)

                        article_datetime = datetime.strptime(date_str.split("T")[0], '%Y-%m-%d').date()

                        if article_datetime > self.date_end:
                            continue

                        if article_datetime < self.date_start:
                            break
                        print(url, title, date_str)
                        extracted.append({
                            "url": self.BASE_URL + url,
                            "title": title,
                            "date_publication": str(article_datetime),
                            "content": None,
                            "source": self.SOURCE_NAME
                        })

        return extracted, True

    def __scroll_and_load(self):
        scroll_count = 0
        continue_loading = True

        while continue_loading:
            start_time = time.time()
            try:
                # –ò—â–µ–º –í–°–ï –∫–Ω–æ–ø–∫–∏ —Å –Ω—É–∂–Ω—ã–º data-test-id
                buttons = self.driver.find_elements(By.CSS_SELECTOR, "div.timeline__more")

                if buttons:
                    button = buttons[0]  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                     # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å
                    self.driver.execute_script("arguments[0].click();", button)
                    print("üîò –ö–Ω–æ–ø–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –∏ –Ω–∞–∂–∞—Ç–∞.")
                    time.sleep(SCROLL_PAUSE_TIME)
                else:
                    self.driver.execute_script("window.scrollBy({top: 2000, left: 0});")
                    print("üîç –ö–Ω–æ–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–∫—Ä–æ–ª–ª –≤–Ω–∏–∑.")

                elapsed = time.time() - start_time
                if elapsed > self.MAX_WAIT_TIME:
                    print(f"‚è± –°—Ç—Ä–∞–Ω–∏—Ü–∞ –≥—Ä—É–∑–∏–ª–∞—Å—å –±–æ–ª–µ–µ {self.MAX_WAIT_TIME} —Å–µ–∫—É–Ω–¥. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ.")
                    break  # –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–∏ —Å–∫—Ä–æ–ª–ª–µ, —Å—Ç–æ—Ä–æ–Ω–∞ —Å–µ—Ä–≤–µ—Ä–∞ –¢–ê–°–°

                if scroll_count % PARSE_INTERVAL == 0:
                    continue_loading = self.__extract_news_items(self.driver.page_source, False)
                    print(continue_loading)

            except NoSuchElementException:
                print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–Ω–æ–ø–æ–∫.")
                continue_loading = False

            scroll_count += 1
            time.sleep(SCROLL_PAUSE_TIME)

        new_items, _ = self.__extract_news_items(self.driver.page_source, True)
        self.news = new_items



    def __run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        self.driver.get(self.url)
        self.__scroll_and_load()
        return self.news


    def __parse_news_page(self, url: str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ requests –∏ BeautifulSoup"""
        try:
            response = requests.get(url, headers={'Accept-Charset': 'utf-8'})
            response.encoding = response.apparent_encoding  # –ó–∞–º–µ–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            soup = BeautifulSoup(response.text, 'html.parser')
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            text_blocks = soup.find_all('p')
            content = '\n'.join([p.text.strip() for p in text_blocks if p.text.strip()])
            return content
        except Exception as e:
            return None

    def news_page(self): # public
        news_urls = self.__run()
        print("–í—ã–∑–æ–≤ –ø–æ—Å–ª–µ")
        for item in news_urls:
            try:
                content = self.__parse_news_page(item['url'])
                item['content'] = content
                print(content)
            except Exception as e:
                print(f"{e}")
        return self.news

# if __name__ == "__main__":
#     url = "https://www.interfax.ru/business/"
#     driver = webdriver.Chrome()
#     date_start = datetime.strptime(
#         DATE_RANGE[0].strip(), "%Y-%m-%d").date()
#     date_end = datetime.strptime(
#         DATE_RANGE[1].strip(), "%Y-%m-%d").date()
#     parser = RIAParser(url=url, date_range = (date_start, date_end), driver=driver)
#     result = parser.news_page()
#     print(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(result)}")
#     for item in result:  # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
#         print(item)